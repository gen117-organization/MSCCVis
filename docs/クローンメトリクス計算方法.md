# クローンメトリクス計算方法

本ドキュメントでは, `src/modules/visualization/compute_clone_metrics.py` における各メトリクスの計算方法を, 実装コードとともに説明する.

## 入力データ

すべてのメトリクスは **enriched_fragments.csv** を基盤として計算する.

```
clone_id, fragment_index, file_path, file_id, service,
start_line, end_line, line_count, file_type,
modified_commits (JSON array), modified_count
```

ROC の分母のみ **services.json** の `language_stats` → サービス別 `total_loc` を使用する.

## 共通ヘルパー

### 同時修正コミットの抽出

「**同時修正 (co-modification)**」は, 同一コミットでクローンセット内の 2 つ以上のフラグメントが修正されている場合と定義する.

```python
def _compute_comod_commits_for_clone_set(
    fragments_df: pd.DataFrame,
) -> set[str]:
    commit_to_fragments: dict[str, set[int]] = {}
    for _, row in fragments_df.iterrows():
        frag_idx = int(row["fragment_index"])
        for commit in _parse_commits(str(row["modified_commits"])):
            commit_to_fragments.setdefault(commit, set()).add(frag_idx)
    return {c for c, frags in commit_to_fragments.items() if len(frags) >= 2}
```

各フラグメントの `modified_commits` (JSON 配列) を展開し, コミットごとに修正されたフラグメントの index を集計する. 2 つ以上のフラグメントが修正されたコミットを「同時修正コミット」として返す.

### 同時修正フラグメントの特定

```python
def _get_comod_fragment_indices(
    fragments_df: pd.DataFrame,
    comod_commits: set[str],
) -> set[int]:
    indices: set[int] = set()
    for _, row in fragments_df.iterrows():
        frag_commits = _parse_commits(str(row["modified_commits"]))
        if frag_commits & comod_commits:
            indices.add(int(row["fragment_index"]))
    return indices
```

同時修正コミットに 1 つでも関与しているフラグメントの index を返す.

### クローンセット → サービスマップ

```python
def _build_clone_set_service_map(
    df: pd.DataFrame,
) -> dict[str, set[str]]:
    result: dict[str, set[str]] = {}
    for clone_id, group in df.groupby("clone_id"):
        services = {s for s in group["service"].unique() if s}
        result[str(clone_id)] = services
    return result
```

全データを 1 回走査し, 各 clone_id が属するサービスの集合を構築する. サービス・ファイル両粒度の計算で共有する.

---

## 1. マイクロサービスベース

計算関数: `_compute_single_service()`. サービスに属する全フラグメント (`svc_df`) を集計する.

### 1.1 マイクロサービスに含まれているクローンセットの数

```python
clone_ids = set(svc_df["clone_id"].unique())
clone_set_count = len(clone_ids)
```

→ `ServiceMetrics.clone_set_count`

そのサービスのフラグメントが属する **ユニークな clone_id** の数.

### 1.2 マイクロサービスに含まれている合計行数

```python
total_clone_line = int(svc_df["line_count"].sum())
```

→ `ServiceMetrics.total_clone_line_count`

サービスに属する全フラグメントの `line_count` の合計.

### 1.3 コードクローンの平均行数

```python
frag_count = len(svc_df)
avg_line = total_clone_line / frag_count if frag_count > 0 else 0.0
```

→ `ServiceMetrics.clone_avg_line_count`

合計クローン行数 (`total_clone_line`) をフラグメント数で割った値.

### 1.4 マイクロサービスに含まれているクローンセットに含まれているファイル数

```python
clone_file_count = int(svc_df["file_path"].nunique())
```

→ `ServiceMetrics.clone_file_count`

サービスのフラグメントが所在する **ユニークな file_path** の数.

### 1.5 ROC（クローンの行の割合）

```python
svc_total_loc = services_loc.get(service, 0)
roc = total_clone_line / svc_total_loc if svc_total_loc > 0 else 0.0
```

→ `ServiceMetrics.roc`

分子 = サービスのクローン合計行数 (`total_clone_line`).
分母 = `services.json` の `language_stats[language]["services"][service]["total_loc"]`.

```python
def _extract_service_loc(language_stats: dict[str, Any]) -> dict[str, int]:
    services_section = language_stats.get("services", {})
    return {
        svc: int(info.get("total_loc", 0))
        for svc, info in services_section.items()
        if isinstance(info, dict)
    }
```

### 1.6 コードクローンに対する同時修正数

```python
comod_count = 0
comod_other_services: set[str] = set()
for cid in clone_ids:
    cs_df = full_df[full_df["clone_id"] == cid]
    comod_commits = _compute_comod_commits_for_clone_set(cs_df)
    comod_count += len(comod_commits)
```

→ `ServiceMetrics.comod_count`

サービスに含まれる各クローンセットについて, クローンセット全体 (`full_df`) から同時修正コミットを抽出し, その **総数** を合計する. 注意: 同時修正の判定はクローンセット全体 (他サービスのフラグメントも含む) で行う.

### 1.7 同時修正されているコードクローンに関わる他のマイクロサービス数

```python
    if comod_commits:
        other_svcs = clone_set_svc_map.get(cid, set()) - {service}
        comod_other_services |= other_svcs

comod_other_service_count = len(comod_other_services)
```

→ `ServiceMetrics.comod_other_service_count`

同時修正が 1 件以上あるクローンセットのうち, 自サービス以外にフラグメントを持つサービスの **ユニーク数**. 全クローンセットにわたって和集合をとる.

---

## 2. クローンセットベース

計算関数: `_compute_single_clone_set()`. 1 つのクローンセットの全フラグメント (`cs_df`) を集計する.

### 2.1 クローンセットが何個のマイクロサービスに跨っているか

```python
services = {s for s in cs_df["service"].unique() if s}
service_count = len(services)
```

→ `CloneSetMetrics.service_count`

空文字 (サービス未解決) を除いたユニークサービス数.

### 2.2 他のサービス間と共有しているコード片の数

```python
is_cross = service_count >= 2

if is_cross:
    cross_frags = cs_df[cs_df["service"] != ""]
    cross_count = len(cross_frags)
else:
    cross_count = 0
```

→ `CloneSetMetrics.cross_service_fragment_count`

2 つ以上のサービスに跨っている場合 (`is_cross`), サービスが解決済みの全フラグメント数. 跨っていない場合は 0.

### 2.3 他のサービス間と共有しているコード片の割合

```python
total_frags = len(cs_df)
cross_ratio = cross_count / total_frags if total_frags > 0 else 0.0
```

→ `CloneSetMetrics.cross_service_fragment_ratio`

`cross_service_fragment_count / 全フラグメント数`.

### 2.4 他のサービス間と共有しているコード片の行数

```python
if is_cross:
    cross_frags = cs_df[cs_df["service"] != ""]
    cross_line = int(cross_frags["line_count"].sum())
else:
    cross_line = 0
```

→ `CloneSetMetrics.cross_service_line_count`

クロスサービスのフラグメントの `line_count` の合計.

### 2.5 他のサービス間と共有しているコード片の数 × 行数（規模感を把握）

```python
cross_scale = cross_count * cross_line
```

→ `CloneSetMetrics.cross_service_scale`

`cross_service_fragment_count × cross_service_line_count`. クロスサービスの「規模感」を 1 つの指標で表す.

### 2.6 マイクロサービスを跨っている要素数

```python
cross_element_count = total_frags if is_cross else 0
```

→ `CloneSetMetrics.cross_service_element_count`

クローンセットがサービスを跨いでいる場合はクローンセット内の全フラグメント数. 跨いでいない場合は 0.

### 2.7 何回同時修正（simultaneous）されているか

```python
comod_commits = _compute_comod_commits_for_clone_set(cs_df)
comod_count = len(comod_commits)
```

→ `CloneSetMetrics.comod_count`

クローンセット内で 2 つ以上のフラグメントが同一コミットで修正された回数 (ユニークなコミット数).

### 2.8 同時修正されているクローンセット内の要素数

```python
comod_frag_indices = _get_comod_fragment_indices(cs_df, comod_commits)
comod_frag_count = len(comod_frag_indices)
```

→ `CloneSetMetrics.comod_fragment_count`

同時修正コミットに 1 つでも関与しているフラグメントのユニーク数.

### 2.9 同時修正されているクローンセット内の要素の割合

```python
comod_frag_ratio = comod_frag_count / total_frags if total_frags > 0 else 0.0
```

→ `CloneSetMetrics.comod_fragment_ratio`

`comod_fragment_count / 全フラグメント数`.

---

## 3. ファイルベース

計算関数: `_compute_single_file()`. 1 つのファイルに属する全フラグメント (`file_df`) を集計する. ファイルのサービスは多数決で決定する:

```python
def _majority_service(file_df: pd.DataFrame) -> str:
    resolved = file_df[file_df["service"] != ""]
    if resolved.empty:
        return ""
    return str(resolved["service"].mode().iloc[0])
```

### 3.1 クローンセットを共有しているマイクロサービスの数

```python
file_service = _majority_service(file_df)
file_clone_ids = set(file_df["clone_id"].unique())

sharing_services: set[str] = set()
for cid in file_clone_ids:
    cs_services = clone_set_svc_map.get(cid, set())
    other_services = cs_services - {file_service} if file_service else cs_services
    if other_services:
        sharing_services |= other_services

sharing_count = len(sharing_services)
```

→ `FileMetrics.sharing_service_count`

ファイルのクローンセットが属するサービスのうち, 自サービスを除いたユニークサービス数.

### 3.2 マイクロサービス数

```python
total_service_count = total_service_count  # 引数として渡される
```

→ `FileMetrics.total_service_count`

プロジェクト全体のサービス数. `compute_file_metrics()` が `language_stats` の `services` 数, または `df` 内のユニークサービス数から決定する:

```python
services_section = lang_stats.get("services", {})
total_svc = (
    len(services_section)
    if services_section
    else len({s for s in df["service"].unique() if s})
)
```

### 3.3 他のマイクロサービスと共有しているクローンセットの数

```python
cross_clone_sets: set[str] = set()
for cid in file_clone_ids:
    cs_services = clone_set_svc_map.get(cid, set())
    other_services = cs_services - {file_service} if file_service else cs_services
    if other_services:
        cross_clone_sets.add(cid)

cross_cs_count = len(cross_clone_sets)
```

→ `FileMetrics.cross_service_clone_set_count`

ファイルのクローンセットのうち, 自サービス以外にもフラグメントを持つクローンセットの数.

### 3.4 他のマイクロサービスと共有しているクローンセットの割合

```python
total_clone_sets = len(file_clone_ids)
cross_cs_ratio = cross_cs_count / total_clone_sets if total_clone_sets > 0 else 0.0
```

→ `FileMetrics.cross_service_clone_set_ratio`

`cross_service_clone_set_count / ファイル内の全クローンセット数`.

### 3.5 クローンセットを共有しているマイクロサービス / 全マイクロサービス

```python
sharing_ratio = (
    sharing_count / total_service_count if total_service_count > 0 else 0.0
)
```

→ `FileMetrics.sharing_service_ratio`

`sharing_service_count / total_service_count`.

### 3.6 他のマイクロサービスと共有しているトークン数（行数）

```python
cross_line = 0
for cid in file_clone_ids:
    cs_services = clone_set_svc_map.get(cid, set())
    other_services = cs_services - {file_service} if file_service else cs_services
    if other_services:
        cid_file_frags = file_df[file_df["clone_id"] == cid]
        cross_line += int(cid_file_frags["line_count"].sum())
```

→ `FileMetrics.cross_service_line_count`

他サービスと共有しているクローンセットに属する, **当該ファイル内の**フラグメントの `line_count` 合計. enriched_fragments.csv に `token_count` が含まれないため, 行数で代替している.

### 3.7 他のマイクロサービスと共有しているクローンセットに関わる同時修正数

```python
cross_comod_count = 0
for cid in cross_clone_sets:
    cs_df = full_df[full_df["clone_id"] == cid]
    comod_commits = _compute_comod_commits_for_clone_set(cs_df)
    cross_comod_count += len(comod_commits)
```

→ `FileMetrics.cross_service_comod_count`

クロスサービスのクローンセットに限定して, 同時修正コミット数を合計する.

### 3.8 コードクローンに関する同時修正を共有したマイクロサービス数

```python
comod_shared_services: set[str] = set()
for cid in cross_clone_sets:
    cs_df = full_df[full_df["clone_id"] == cid]
    comod_commits = _compute_comod_commits_for_clone_set(cs_df)
    if comod_commits:
        other_svcs = clone_set_svc_map.get(cid, set()) - {file_service}
        comod_shared_services |= other_svcs

comod_shared_service_count = len(comod_shared_services)
```

→ `FileMetrics.comod_shared_service_count`

同時修正が存在するクロスサービスのクローンセットの中で, 自サービス以外のサービスのユニーク数.
